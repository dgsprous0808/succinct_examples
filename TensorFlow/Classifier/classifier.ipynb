{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Classification Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be working with some California Census Data, we'll be trying to use various features of an individual to predict what class of income they belogn in (>50k or <=50k). \n",
    "\n",
    "Here is some information about the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "<th>Column Name</th>\n",
    "<th>Type</th>\n",
    "<th>Description</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>age</td>\n",
    "<td>Continuous</td>\n",
    "<td>The age of the individual</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>workclass</td>\n",
    "<td>Categorical</td>\n",
    "<td>The type of employer the  individual has (government,  military, private, etc.).</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>fnlwgt</td>\n",
    "<td>Continuous</td>\n",
    "<td>The number of people the census  takers believe that observation  represents (sample weight). This  variable will not be used.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>education</td>\n",
    "<td>Categorical</td>\n",
    "<td>The highest level of education  achieved for that individual.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>education_num</td>\n",
    "<td>Continuous</td>\n",
    "<td>The highest level of education in  numerical form.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>marital_status</td>\n",
    "<td>Categorical</td>\n",
    "<td>Marital status of the individual.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>occupation</td>\n",
    "<td>Categorical</td>\n",
    "<td>The occupation of the individual.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>relationship</td>\n",
    "<td>Categorical</td>\n",
    "<td>Wife, Own-child, Husband,  Not-in-family, Other-relative,  Unmarried.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>race</td>\n",
    "<td>Categorical</td>\n",
    "<td>White, Asian-Pac-Islander,  Amer-Indian-Eskimo, Other, Black.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>gender</td>\n",
    "<td>Categorical</td>\n",
    "<td>Female, Male.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>capital_gain</td>\n",
    "<td>Continuous</td>\n",
    "<td>Capital gains recorded.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>capital_loss</td>\n",
    "<td>Continuous</td>\n",
    "<td>Capital Losses recorded.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>hours_per_week</td>\n",
    "<td>Continuous</td>\n",
    "<td>Hours worked per week.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>native_country</td>\n",
    "<td>Categorical</td>\n",
    "<td>Country of origin of the  individual.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>income</td>\n",
    "<td>Categorical</td>\n",
    "<td>\"&gt;50K\" or \"&lt;=50K\", meaning  whether the person makes more  than \\$50,000 annually.</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Follow the Directions in Bold. If you get stuck, check out the solutions lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THE DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Read in the census_data.csv data with pandas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>education</th>\n",
       "      <th>education_num</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>native_country</th>\n",
       "      <th>income_bracket</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          workclass   education  education_num       marital_status  \\\n",
       "0   39          State-gov   Bachelors             13        Never-married   \n",
       "1   50   Self-emp-not-inc   Bachelors             13   Married-civ-spouse   \n",
       "2   38            Private     HS-grad              9             Divorced   \n",
       "3   53            Private        11th              7   Married-civ-spouse   \n",
       "4   28            Private   Bachelors             13   Married-civ-spouse   \n",
       "\n",
       "           occupation    relationship    race   gender  capital_gain  \\\n",
       "0        Adm-clerical   Not-in-family   White     Male          2174   \n",
       "1     Exec-managerial         Husband   White     Male             0   \n",
       "2   Handlers-cleaners   Not-in-family   White     Male             0   \n",
       "3   Handlers-cleaners         Husband   Black     Male             0   \n",
       "4      Prof-specialty            Wife   Black   Female             0   \n",
       "\n",
       "   capital_loss  hours_per_week  native_country income_bracket  \n",
       "0             0              40   United-States          <=50K  \n",
       "1             0              13   United-States          <=50K  \n",
       "2             0              40   United-States          <=50K  \n",
       "3             0              40   United-States          <=50K  \n",
       "4             0              40            Cuba          <=50K  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('census_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** TensorFlow won't be able to understand strings as labels, you'll need to use pandas .apply() method to apply a custom function that converts them to 0s and 1s. This might be hard if you aren't very familiar with pandas, so feel free to take a peek at the solutions for this part.**\n",
    "\n",
    "** Convert the Label column to 0s and 1s instead of strings.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform a Train Test Split on the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a label\n",
    "# NB:  ' >50K' not '>50K'\n",
    "df['label'] = 0\n",
    "df.loc[df['income_bracket']==\" >50K\",'label'] = 1\n",
    "\n",
    "# split the data\n",
    "y_val = df['label']\n",
    "x_data = df.drop('income_bracket',axis=1).drop('label',axis=1)\n",
    "\n",
    "test_size, seedval = .30, 101\n",
    "\n",
    "# I don't like his notation but I am adopting.  I would normally have X_test or X_train\n",
    "# as df_test & df_train\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data,y_val,\n",
    "                                        test_size=test_size,random_state=seedval)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>education</th>\n",
       "      <th>education_num</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>native_country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          workclass   education  education_num       marital_status  \\\n",
       "0   39          State-gov   Bachelors             13        Never-married   \n",
       "1   50   Self-emp-not-inc   Bachelors             13   Married-civ-spouse   \n",
       "2   38            Private     HS-grad              9             Divorced   \n",
       "3   53            Private        11th              7   Married-civ-spouse   \n",
       "4   28            Private   Bachelors             13   Married-civ-spouse   \n",
       "\n",
       "           occupation    relationship    race   gender  capital_gain  \\\n",
       "0        Adm-clerical   Not-in-family   White     Male          2174   \n",
       "1     Exec-managerial         Husband   White     Male             0   \n",
       "2   Handlers-cleaners   Not-in-family   White     Male             0   \n",
       "3   Handlers-cleaners         Husband   Black     Male             0   \n",
       "4      Prof-specialty            Wife   Black   Female             0   \n",
       "\n",
       "   capital_loss  hours_per_week  native_country  \n",
       "0             0              40   United-States  \n",
       "1             0              13   United-States  \n",
       "2             0              40   United-States  \n",
       "3             0              40   United-States  \n",
       "4             0              40            Cuba  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Feature Columns for tf.esitmator\n",
    "\n",
    "** Take note of categorical vs continuous values! **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cats\n",
    "gender = tf.feature_column.categorical_column_with_vocabulary_list(\"gender\", [\"Female\", \"Male\"])\n",
    "occupation = tf.feature_column.categorical_column_with_hash_bucket(\"occupation\", hash_bucket_size=1000)\n",
    "marital_status = tf.feature_column.categorical_column_with_hash_bucket(\"marital_status\", hash_bucket_size=1000)\n",
    "relationship = tf.feature_column.categorical_column_with_hash_bucket(\"relationship\", hash_bucket_size=1000)\n",
    "education = tf.feature_column.categorical_column_with_hash_bucket(\"education\", hash_bucket_size=1000)\n",
    "workclass = tf.feature_column.categorical_column_with_hash_bucket(\"workclass\", hash_bucket_size=1000)\n",
    "native_country = tf.feature_column.categorical_column_with_hash_bucket(\"native_country\", hash_bucket_size=1000)\n",
    "\n",
    "# numerics\n",
    "age = tf.feature_column.numeric_column(\"age\")\n",
    "education_num = tf.feature_column.numeric_column(\"education_num\")\n",
    "capital_gain = tf.feature_column.numeric_column(\"capital_gain\")\n",
    "capital_loss = tf.feature_column.numeric_column(\"capital_loss\")\n",
    "hours_per_week = tf.feature_column.numeric_column(\"hours_per_week\")\n",
    "\n",
    "feat_cols = [gender,occupation,marital_status,relationship,education,workclass,native_country,\n",
    "            age,education_num,capital_gain,capital_loss,hours_per_week]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Input Function\n",
    "\n",
    "** Batch_size is up to you. But do make sure to shuffle!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_func_train=tf.estimator.inputs.pandas_input_fn(\n",
    "    x=X_train,y=y_train,batch_size=250,\n",
    "    num_epochs=None,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create your model with tf.estimator\n",
    "\n",
    "**Create a LinearClassifier.(If you want to use a DNNClassifier, keep in mind you'll need to create embedded columns out of the cateogrical feature that use strings, check out the previous lecture on this for more info.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/9y/n0rxh9rj60q4v8zgsclp0gm97tm33p/T/tmp3gnxeb2c\n",
      "INFO:tensorflow:Using config: {'_log_step_count_steps': 100, '_save_checkpoints_steps': None, '_save_summary_steps': 100, '_keep_checkpoint_every_n_hours': 10000, '_tf_random_seed': 1, '_model_dir': '/var/folders/9y/n0rxh9rj60q4v8zgsclp0gm97tm33p/T/tmp3gnxeb2c', '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5}\n"
     ]
    }
   ],
   "source": [
    "LCM_model = tf.estimator.LinearClassifier(feature_columns=feat_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Train your model on the data, for at least 5000 steps. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/9y/n0rxh9rj60q4v8zgsclp0gm97tm33p/T/tmp3gnxeb2c/model.ckpt.\n",
      "INFO:tensorflow:loss = 173.287, step = 1\n",
      "INFO:tensorflow:global_step/sec: 172.837\n",
      "INFO:tensorflow:loss = 820.552, step = 101 (0.580 sec)\n",
      "INFO:tensorflow:global_step/sec: 187.308\n",
      "INFO:tensorflow:loss = 439.485, step = 201 (0.534 sec)\n",
      "INFO:tensorflow:global_step/sec: 235.911\n",
      "INFO:tensorflow:loss = 134.331, step = 301 (0.424 sec)\n",
      "INFO:tensorflow:global_step/sec: 252.953\n",
      "INFO:tensorflow:loss = 135.146, step = 401 (0.395 sec)\n",
      "INFO:tensorflow:global_step/sec: 269.157\n",
      "INFO:tensorflow:loss = 264.662, step = 501 (0.372 sec)\n",
      "INFO:tensorflow:global_step/sec: 253.213\n",
      "INFO:tensorflow:loss = 651.25, step = 601 (0.395 sec)\n",
      "INFO:tensorflow:global_step/sec: 230.185\n",
      "INFO:tensorflow:loss = 102.806, step = 701 (0.435 sec)\n",
      "INFO:tensorflow:global_step/sec: 186.469\n",
      "INFO:tensorflow:loss = 138.464, step = 801 (0.536 sec)\n",
      "INFO:tensorflow:global_step/sec: 187.513\n",
      "INFO:tensorflow:loss = 103.007, step = 901 (0.533 sec)\n",
      "INFO:tensorflow:global_step/sec: 187.144\n",
      "INFO:tensorflow:loss = 251.681, step = 1001 (0.534 sec)\n",
      "INFO:tensorflow:global_step/sec: 187.01\n",
      "INFO:tensorflow:loss = 132.923, step = 1101 (0.535 sec)\n",
      "INFO:tensorflow:global_step/sec: 216.302\n",
      "INFO:tensorflow:loss = 688.528, step = 1201 (0.462 sec)\n",
      "INFO:tensorflow:global_step/sec: 187.469\n",
      "INFO:tensorflow:loss = 365.297, step = 1301 (0.533 sec)\n",
      "INFO:tensorflow:global_step/sec: 185.554\n",
      "INFO:tensorflow:loss = 224.128, step = 1401 (0.539 sec)\n",
      "INFO:tensorflow:global_step/sec: 187.225\n",
      "INFO:tensorflow:loss = 290.178, step = 1501 (0.534 sec)\n",
      "INFO:tensorflow:global_step/sec: 187.148\n",
      "INFO:tensorflow:loss = 216.843, step = 1601 (0.534 sec)\n",
      "INFO:tensorflow:global_step/sec: 187.078\n",
      "INFO:tensorflow:loss = 112.637, step = 1701 (0.535 sec)\n",
      "INFO:tensorflow:global_step/sec: 186.709\n",
      "INFO:tensorflow:loss = 177.7, step = 1801 (0.536 sec)\n",
      "INFO:tensorflow:global_step/sec: 188.449\n",
      "INFO:tensorflow:loss = 96.9993, step = 1901 (0.531 sec)\n",
      "INFO:tensorflow:global_step/sec: 185.694\n",
      "INFO:tensorflow:loss = 76.792, step = 2001 (0.539 sec)\n",
      "INFO:tensorflow:global_step/sec: 187.208\n",
      "INFO:tensorflow:loss = 551.998, step = 2101 (0.534 sec)\n",
      "INFO:tensorflow:global_step/sec: 187.975\n",
      "INFO:tensorflow:loss = 387.905, step = 2201 (0.532 sec)\n",
      "INFO:tensorflow:global_step/sec: 241.306\n",
      "INFO:tensorflow:loss = 119.425, step = 2301 (0.414 sec)\n",
      "INFO:tensorflow:global_step/sec: 227.046\n",
      "INFO:tensorflow:loss = 86.446, step = 2401 (0.440 sec)\n",
      "INFO:tensorflow:global_step/sec: 248.733\n",
      "INFO:tensorflow:loss = 112.695, step = 2501 (0.402 sec)\n",
      "INFO:tensorflow:global_step/sec: 186.396\n",
      "INFO:tensorflow:loss = 109.54, step = 2601 (0.536 sec)\n",
      "INFO:tensorflow:global_step/sec: 187.92\n",
      "INFO:tensorflow:loss = 122.196, step = 2701 (0.532 sec)\n",
      "INFO:tensorflow:global_step/sec: 186.36\n",
      "INFO:tensorflow:loss = 96.7621, step = 2801 (0.537 sec)\n",
      "INFO:tensorflow:global_step/sec: 184.165\n",
      "INFO:tensorflow:loss = 266.232, step = 2901 (0.543 sec)\n",
      "INFO:tensorflow:global_step/sec: 187.517\n",
      "INFO:tensorflow:loss = 109.155, step = 3001 (0.534 sec)\n",
      "INFO:tensorflow:global_step/sec: 186.232\n",
      "INFO:tensorflow:loss = 105.383, step = 3101 (0.537 sec)\n",
      "INFO:tensorflow:global_step/sec: 180.944\n",
      "INFO:tensorflow:loss = 84.911, step = 3201 (0.553 sec)\n",
      "INFO:tensorflow:global_step/sec: 169.39\n",
      "INFO:tensorflow:loss = 76.6664, step = 3301 (0.590 sec)\n",
      "INFO:tensorflow:global_step/sec: 183.204\n",
      "INFO:tensorflow:loss = 266.896, step = 3401 (0.546 sec)\n",
      "INFO:tensorflow:global_step/sec: 240.508\n",
      "INFO:tensorflow:loss = 107.863, step = 3501 (0.416 sec)\n",
      "INFO:tensorflow:global_step/sec: 173.843\n",
      "INFO:tensorflow:loss = 291.103, step = 3601 (0.575 sec)\n",
      "INFO:tensorflow:global_step/sec: 178.565\n",
      "INFO:tensorflow:loss = 189.114, step = 3701 (0.560 sec)\n",
      "INFO:tensorflow:global_step/sec: 172.314\n",
      "INFO:tensorflow:loss = 196.608, step = 3801 (0.581 sec)\n",
      "INFO:tensorflow:global_step/sec: 169.672\n",
      "INFO:tensorflow:loss = 89.9356, step = 3901 (0.589 sec)\n",
      "INFO:tensorflow:global_step/sec: 171.083\n",
      "INFO:tensorflow:loss = 712.567, step = 4001 (0.584 sec)\n",
      "INFO:tensorflow:global_step/sec: 171.855\n",
      "INFO:tensorflow:loss = 160.693, step = 4101 (0.582 sec)\n",
      "INFO:tensorflow:global_step/sec: 217.179\n",
      "INFO:tensorflow:loss = 423.953, step = 4201 (0.461 sec)\n",
      "INFO:tensorflow:global_step/sec: 177.297\n",
      "INFO:tensorflow:loss = 364.858, step = 4301 (0.564 sec)\n",
      "INFO:tensorflow:global_step/sec: 209.628\n",
      "INFO:tensorflow:loss = 92.2788, step = 4401 (0.477 sec)\n",
      "INFO:tensorflow:global_step/sec: 191.498\n",
      "INFO:tensorflow:loss = 70.6505, step = 4501 (0.522 sec)\n",
      "INFO:tensorflow:global_step/sec: 194.786\n",
      "INFO:tensorflow:loss = 191.902, step = 4601 (0.514 sec)\n",
      "INFO:tensorflow:global_step/sec: 185.378\n",
      "INFO:tensorflow:loss = 116.657, step = 4701 (0.539 sec)\n",
      "INFO:tensorflow:global_step/sec: 233.043\n",
      "INFO:tensorflow:loss = 104.282, step = 4801 (0.429 sec)\n",
      "INFO:tensorflow:global_step/sec: 241.32\n",
      "INFO:tensorflow:loss = 299.683, step = 4901 (0.415 sec)\n",
      "INFO:tensorflow:global_step/sec: 257.67\n",
      "INFO:tensorflow:loss = 190.497, step = 5001 (0.388 sec)\n",
      "INFO:tensorflow:global_step/sec: 211.98\n",
      "INFO:tensorflow:loss = 120.4, step = 5101 (0.472 sec)\n",
      "INFO:tensorflow:global_step/sec: 178.264\n",
      "INFO:tensorflow:loss = 143.249, step = 5201 (0.561 sec)\n",
      "INFO:tensorflow:global_step/sec: 198.629\n",
      "INFO:tensorflow:loss = 506.342, step = 5301 (0.503 sec)\n",
      "INFO:tensorflow:global_step/sec: 169.389\n",
      "INFO:tensorflow:loss = 151.385, step = 5401 (0.590 sec)\n",
      "INFO:tensorflow:global_step/sec: 176.888\n",
      "INFO:tensorflow:loss = 92.525, step = 5501 (0.565 sec)\n",
      "INFO:tensorflow:global_step/sec: 179.537\n",
      "INFO:tensorflow:loss = 70.249, step = 5601 (0.557 sec)\n",
      "INFO:tensorflow:global_step/sec: 177.46\n",
      "INFO:tensorflow:loss = 96.4213, step = 5701 (0.564 sec)\n",
      "INFO:tensorflow:global_step/sec: 199.087\n",
      "INFO:tensorflow:loss = 172.777, step = 5801 (0.502 sec)\n",
      "INFO:tensorflow:global_step/sec: 202.537\n",
      "INFO:tensorflow:loss = 207.142, step = 5901 (0.494 sec)\n",
      "INFO:tensorflow:global_step/sec: 231.879\n",
      "INFO:tensorflow:loss = 90.5459, step = 6001 (0.431 sec)\n",
      "INFO:tensorflow:global_step/sec: 242.713\n",
      "INFO:tensorflow:loss = 285.122, step = 6101 (0.412 sec)\n",
      "INFO:tensorflow:global_step/sec: 248.483\n",
      "INFO:tensorflow:loss = 107.445, step = 6201 (0.402 sec)\n",
      "INFO:tensorflow:global_step/sec: 200.486\n",
      "INFO:tensorflow:loss = 152.135, step = 6301 (0.499 sec)\n",
      "INFO:tensorflow:global_step/sec: 238.172\n",
      "INFO:tensorflow:loss = 72.404, step = 6401 (0.420 sec)\n",
      "INFO:tensorflow:global_step/sec: 203.085\n",
      "INFO:tensorflow:loss = 83.9313, step = 6501 (0.492 sec)\n",
      "INFO:tensorflow:global_step/sec: 227.735\n",
      "INFO:tensorflow:loss = 100.175, step = 6601 (0.439 sec)\n",
      "INFO:tensorflow:global_step/sec: 244.602\n",
      "INFO:tensorflow:loss = 295.788, step = 6701 (0.409 sec)\n",
      "INFO:tensorflow:global_step/sec: 174.281\n",
      "INFO:tensorflow:loss = 81.2505, step = 6801 (0.574 sec)\n",
      "INFO:tensorflow:global_step/sec: 170.132\n",
      "INFO:tensorflow:loss = 126.536, step = 6901 (0.588 sec)\n",
      "INFO:tensorflow:global_step/sec: 169.851\n",
      "INFO:tensorflow:loss = 113.18, step = 7001 (0.589 sec)\n",
      "INFO:tensorflow:global_step/sec: 181.658\n",
      "INFO:tensorflow:loss = 199.115, step = 7101 (0.551 sec)\n",
      "INFO:tensorflow:global_step/sec: 191.255\n",
      "INFO:tensorflow:loss = 84.716, step = 7201 (0.523 sec)\n",
      "INFO:tensorflow:global_step/sec: 176.232\n",
      "INFO:tensorflow:loss = 125.919, step = 7301 (0.567 sec)\n",
      "INFO:tensorflow:global_step/sec: 181.822\n",
      "INFO:tensorflow:loss = 152.759, step = 7401 (0.550 sec)\n",
      "INFO:tensorflow:global_step/sec: 201.685\n",
      "INFO:tensorflow:loss = 114.976, step = 7501 (0.496 sec)\n",
      "INFO:tensorflow:global_step/sec: 176.984\n",
      "INFO:tensorflow:loss = 83.9296, step = 7601 (0.565 sec)\n",
      "INFO:tensorflow:global_step/sec: 203.503\n",
      "INFO:tensorflow:loss = 86.7895, step = 7701 (0.491 sec)\n",
      "INFO:tensorflow:global_step/sec: 227.469\n",
      "INFO:tensorflow:loss = 100.338, step = 7801 (0.440 sec)\n",
      "INFO:tensorflow:global_step/sec: 233.407\n",
      "INFO:tensorflow:loss = 385.824, step = 7901 (0.428 sec)\n",
      "INFO:tensorflow:global_step/sec: 218.573\n",
      "INFO:tensorflow:loss = 96.9284, step = 8001 (0.458 sec)\n",
      "INFO:tensorflow:global_step/sec: 183.506\n",
      "INFO:tensorflow:loss = 95.5368, step = 8101 (0.546 sec)\n",
      "INFO:tensorflow:global_step/sec: 184.129\n",
      "INFO:tensorflow:loss = 92.9712, step = 8201 (0.542 sec)\n",
      "INFO:tensorflow:global_step/sec: 232.769\n",
      "INFO:tensorflow:loss = 196.257, step = 8301 (0.430 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 236.85\n",
      "INFO:tensorflow:loss = 328.908, step = 8401 (0.422 sec)\n",
      "INFO:tensorflow:global_step/sec: 236.555\n",
      "INFO:tensorflow:loss = 83.9438, step = 8501 (0.423 sec)\n",
      "INFO:tensorflow:global_step/sec: 180.52\n",
      "INFO:tensorflow:loss = 299.983, step = 8601 (0.554 sec)\n",
      "INFO:tensorflow:global_step/sec: 244.207\n",
      "INFO:tensorflow:loss = 315.083, step = 8701 (0.409 sec)\n",
      "INFO:tensorflow:global_step/sec: 236.924\n",
      "INFO:tensorflow:loss = 151.669, step = 8801 (0.422 sec)\n",
      "INFO:tensorflow:global_step/sec: 197.342\n",
      "INFO:tensorflow:loss = 79.0574, step = 8901 (0.506 sec)\n",
      "INFO:tensorflow:global_step/sec: 236.592\n",
      "INFO:tensorflow:loss = 175.139, step = 9001 (0.423 sec)\n",
      "INFO:tensorflow:global_step/sec: 238.578\n",
      "INFO:tensorflow:loss = 136.051, step = 9101 (0.419 sec)\n",
      "INFO:tensorflow:global_step/sec: 246.512\n",
      "INFO:tensorflow:loss = 87.0456, step = 9201 (0.406 sec)\n",
      "INFO:tensorflow:global_step/sec: 175.25\n",
      "INFO:tensorflow:loss = 205.826, step = 9301 (0.571 sec)\n",
      "INFO:tensorflow:global_step/sec: 182.236\n",
      "INFO:tensorflow:loss = 93.8667, step = 9401 (0.548 sec)\n",
      "INFO:tensorflow:global_step/sec: 195.521\n",
      "INFO:tensorflow:loss = 69.7873, step = 9501 (0.512 sec)\n",
      "INFO:tensorflow:global_step/sec: 249.143\n",
      "INFO:tensorflow:loss = 84.3823, step = 9601 (0.401 sec)\n",
      "INFO:tensorflow:global_step/sec: 228.91\n",
      "INFO:tensorflow:loss = 99.0299, step = 9701 (0.437 sec)\n",
      "INFO:tensorflow:global_step/sec: 250.428\n",
      "INFO:tensorflow:loss = 78.6665, step = 9801 (0.399 sec)\n",
      "INFO:tensorflow:global_step/sec: 246.72\n",
      "INFO:tensorflow:loss = 79.1671, step = 9901 (0.405 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 10000 into /var/folders/9y/n0rxh9rj60q4v8zgsclp0gm97tm33p/T/tmp3gnxeb2c/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 88.9253.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.linear.LinearClassifier at 0x11e077908>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LCM_model.train(input_fn=input_func_train,steps=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Use model.predict() and pass in your input function. This will produce a generator of predictions, which you can then transform into a list, with list() **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_func_test = tf.estimator.inputs.pandas_input_fn(\n",
    "    x=X_test,\n",
    "    batch_size=len(X_test),\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /var/folders/9y/n0rxh9rj60q4v8zgsclp0gm97tm33p/T/tmp3gnxeb2c/model.ckpt-10000\n"
     ]
    }
   ],
   "source": [
    "predictions = list( LCM_model.predict(input_func_test) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Create a list of only the class_ids key values from the prediction list of dictionaries, these are the predictions you will use to compare against the real y_test values. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for prediction in predictions:\n",
    "    results.append( prediction['class_ids'][0] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32561"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Import classification_report from sklearn.metrics and then see if you can figure out how to use it to easily get a full report of your model's performance on the test data. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use notation of the function as documented.\n",
    "y_true, y_pred = y_test, results\n",
    "cr = classification_report(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.90      0.90      7436\n",
      "          1       0.67      0.65      0.66      2333\n",
      "\n",
      "avg / total       0.84      0.84      0.84      9769\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Great Job!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
